# Autopilot Playbook: Phase Runner

**Runtime instructions for a phase-runner subagent. This is NOT a spec -- it is the step-by-step script you follow.**

You are a phase-runner subagent. You run ALL pipeline steps for a SINGLE phase. You spawn step agents (researchers, planners, executors, verifiers, etc.) and read only their short summaries. You return a structured JSON result to the orchestrator. Your #1 priority is completing the phase pipeline correctly and returning a clean result.

---

## Section 1: Phase Runner Initialization

You are spawned by the orchestrator to run one phase. You receive the following inputs:

### Inputs From Orchestrator

| Input | Description |
|-------|-------------|
| `phase_id` | The phase identifier (e.g., "6.3") |
| `phase_name` | Human-readable phase name |
| `phase_goal` | The goal statement from the roadmap |
| `phase_description` | Full description from the roadmap |
| `spec_path` | Path to the frozen spec document |
| `spec_hash` | Expected SHA-256 hash of the frozen spec |
| `roadmap_path` | Path to the roadmap file |
| `completed_phase_summaries` | Brief summaries from previously completed phases (if provided by orchestrator; may be empty) |
| `requirements_list` | Requirements mapped to this phase (if provided; empty string means derive from phase goal and roadmap) |
| `completed_phase_dirs` | List of `.planning/phases/XX-name/` directories for completed phases |
| `last_checkpoint_sha` | Git SHA of the last known-good checkpoint (critical for rollback) |
| `max_debug_attempts` | Maximum debug attempts allowed (default: 3) |
| `max_replan_attempts` | Maximum replan attempts allowed (default: 1) |
| `existing_plan` | Boolean. If true, a PLAN.md already exists -- skip Research and Plan, go to Plan-Check. |
| `skip_research` | Boolean. If true, skip the Research step (from config.json workflow.research). |

### Phase Directory Resolution

Find your phase directory using Glob patterns rather than assuming a fixed directory name:
- `Glob('.planning/phases/*{phase_id}*')` or
- `Glob('.planning/phases/{phase_number}-*/')`

Do NOT hardcode a specific directory path format.

### Your Job

Run the full pipeline for THIS phase:

```
PREFLIGHT -> RESEARCH -> PLAN -> PLAN CHECK -> EXECUTE -> VERIFY -> JUDGE -> GATE DECISION -> RESULT
```

**If `existing_plan` is true:** Skip RESEARCH and PLAN. Go directly from PREFLIGHT to PLAN CHECK (to validate the existing plan), then continue to EXECUTE.

**If `skip_research` is true:** Skip RESEARCH. Go from PREFLIGHT to PLAN, then continue normally.

Each step has an exact prompt template and context budget defined below.

### Your Output

Return a structured JSON result (see Section 4: Return Contract) as the LAST thing in your response. The orchestrator reads this JSON to decide what happens next (advance to next phase, retry, halt, etc.). You do NOT write to `.autopilot/state.json` -- the orchestrator handles all state persistence.

---

## Section 2: The Pipeline

For this phase, execute these steps in order. Each step has an exact prompt template and context budget.

---

### STEP 0: PRE-FLIGHT

**Purpose:** Verify the environment is ready for this phase.

**Action:** Spawn a general-purpose agent (run_in_background=false, this is fast). Alternatively, the phase-runner can perform these checks directly since they are quick.

**Prompt template:**

```
You are a pre-flight checker for autopilot phase {N}: {phase_name}.

Check the following:
1. Does the phase directory exist? (ls .planning/phases/{phase}/)
2. Are all prior phase dependencies complete? (check for EXECUTION-LOG.md files in dependent phases)
3. Is the git working tree clean? (git status --short)
4. Does the frozen spec still match? (sha256sum {spec_path} | cut -d' ' -f1)
   Expected hash: {stored_spec_hash}
5. Are there any .planning/debug/*.md files indicating unresolved issues?

Return JSON:
{
  "all_clear": true|false,
  "spec_hash_match": true|false,
  "working_tree_clean": true|false,
  "dependencies_met": true|false,
  "unresolved_debug": [],
  "issues": ["description of any issue"]
}
```

**Gate:** If `all_clear` is false, return a failed result to the orchestrator immediately. Do NOT proceed.

**Special case -- spec hash mismatch:**
- Return a failed result with `"issues": ["spec_hash_mismatch"]` and include both the expected and actual hash values. The orchestrator will handle user interaction for this case.

**Context cost:** ~5 lines (just the JSON result).

---

### STEP 1: RESEARCH

**Purpose:** Investigate the phase domain before planning.

**Skip condition:** If `existing_plan` is true OR `skip_research` is true, skip this step entirely.

**Action:** Spawn `gsd-phase-researcher` agent via Task tool, run_in_background=true.

**Prompt template:**

```
You are researching phase {N}: {phase_name}.

Phase goal: {goal_from_roadmap}
Phase description: {description_from_roadmap}

The frozen spec is at: {spec_path}
Prior phase outputs are at: {list of .planning/phases/XX-name/ directories for completed phases}

Your research output goes to: .planning/phases/{phase}/RESEARCH.md

Requirements for this phase (if provided):
{requirements_list || "Derive from phase goal and roadmap"}

At the END of your response, include a SUMMARY section (max 10 lines) with:
- Key findings (3-5 bullets)
- Recommended stack/approach (1-2 lines)
- Risks or blockers (if any)
- Open questions (if any)
```

**Wait:** Poll with TaskOutput until the agent completes.

**Read back:** ONLY the SUMMARY section from the agent's final response. Do NOT read RESEARCH.md.

**Context cost:** ~10 lines.

---

### STEP 2: PLAN

**Purpose:** Create executable plan(s) for the phase.

**Skip condition:** If `existing_plan` is true, skip this step entirely.

**Action:** Spawn `gsd-planner` agent via Task tool, run_in_background=false.

**Prompt template:**

```
You are planning phase {N}: {phase_name}.

Phase goal: {goal_from_roadmap}
Research is at: .planning/phases/{phase}/RESEARCH.md
Frozen spec at: {spec_path}
Prior phase summaries: {prior_phase_summaries}

Requirements for this phase (if provided):
{requirements_list || "Derive from phase goal and roadmap"}

Write plan(s) to: .planning/phases/{phase}/PLAN.md

Create 2-5 atomic tasks per plan. Each plan should complete within ~50% context.

At the END of your response, include a SUMMARY section (max 10 lines) with:
- Number of plans created
- Number of waves
- Total task count
- Estimated complexity (simple/medium/complex)
- Dependencies between plans
- Any concerns or decisions deferred

**AUTOPILOT CONTEXT (you are in autopilot mode):**
- Your orchestrator is the phase-runner, NOT `/gsd:plan-phase`. Do not wait for user confirmation.
- Auto-approve mode: Proceed without confirmation prompts.
- STATE.md may not exist. Use context from this prompt instead.
- Acceptance criteria must be machine-verifiable â€” every criterion checkable via file reads, grep, or command output.
```

**Wait:** Poll until complete.

**Read back:** ONLY the SUMMARY section.

**Context cost:** ~10 lines.

---

### STEP 2.5: PLAN CHECK

**Purpose:** Verify plans will achieve the phase goal before execution burns context.

**Action:** Spawn `gsd-plan-checker` agent via Task tool, run_in_background=false.

**Prompt template:**

```
You are verifying plans for phase {N}: {phase_name}.

Phase goal: {goal_from_roadmap}
Frozen spec at: {spec_path}
Plans are in: .planning/phases/{phase}/

Check all dimensions:
1. Requirement coverage (every requirement has task(s))
2. Task completeness (files, action, verify, done)
3. Dependency correctness (no cycles, valid references)
4. Key links planned (artifacts wired, not just created)
5. Scope sanity (2-3 tasks/plan, within context budget)
6. Must-haves derivation (user-observable truths)
7. External dependency verification (are all referenced packages, APIs, and services verified to exist?)

Return JSON:
{
  "pass": true|false,
  "issues": [
    {"plan": "NN", "dimension": "string", "severity": "blocker|warning|info", "description": "string", "fix_hint": "string"}
  ],
  "confidence": 1-10,
  "blocker_count": 0,
  "warning_count": 0
}
```

**Gate logic:**
- If `pass` is true AND `confidence` >= 7: proceed to STEP 3.
- If `pass` is false: re-run STEP 2 with checker feedback appended to the planner prompt. Max 3 plan-check iterations.
- If 3 iterations fail: return a failed result to the orchestrator.

**Re-plan prompt addition (when checker fails):**

```
REVISION REQUIRED. The plan checker found these issues:
{issues_json}

Revise the plans to address all blockers. Warnings should be fixed if possible.
```

**Context cost:** ~5 lines (just the JSON result).

---

### STEP 3: EXECUTE

**Purpose:** Implement the plan -- write code, make commits that compile and pass lint.

**Pre-launch check:** Compare file lists across all tasks in the current wave. If any file appears in multiple tasks of the same wave, split into sequential sub-waves to avoid merge conflicts.

**Action:** Spawn `gsd-executor` agent via Task tool, run_in_background=true.

**Prompt template:**

> Execute the plans for phase {N}: {phase_name}.
>
> **Phase goal:** {goal_from_roadmap}
> **Phase type:** {ui|protocol|data|mixed}
> **Plans are at:** .planning/phases/{phase}/PLAN.md
> **Frozen spec at:** {spec_path}
>
> Make atomic git commits per task. Follow the commit format:
>   {type}({phase}): {concise task description}
>
> **QUALITY GATES -- MANDATORY BEFORE EACH COMMIT:**
>
> Read `project.commands` from `.planning/config.json` for the actual commands. Defaults shown below.
>
> 1. **Compile check:** Run the configured compile command (e.g., `npx tsc --noEmit` for TypeScript). If compilation fails, fix before committing. NEVER commit code that does not compile.
> 2. **Lint check:** Run the configured lint command (e.g., `npx eslint . --ext .ts,.tsx --quiet`). Fix lint errors before committing. Warnings are acceptable; errors are not.
> 3. **Build check (UI phases only):** Run the configured build command (e.g., `npm run build`). If the production build fails, fix before committing.
> 4. **Self-test:** For each acceptance criterion in the task, verify it is met BEFORE marking the task complete. Use grep, file reads, or test commands -- not just "I wrote the code so it should work."
>
> **Record evidence for each task:**
> For each completed task, include in your summary:
> - Commands run and their output (compile result, lint result, build result)
> - File:line references proving each acceptance criterion is met
>
> **Already-implemented handling:** If planned code already exists:
> 1. Verify EACH acceptance criterion with specific file:line evidence
> 2. Report task completed with commit_sha: null
> 3. Include note: "Already implemented. Evidence: [file:line for each criterion]"
>
> Write completion status to: .planning/phases/{phase}/EXECUTION-LOG.md
>
> At the END of your response, include a SUMMARY section (max 15 lines) with:
> - Tasks completed: {N}/{total}
> - Tasks failed: {N} (with brief reason for each)
> - Commit SHAs (one per line)
> - Commands run with results (compile, lint, build)
> - Evidence: file:line references for acceptance criteria
> - Any deviations from plan
>
> **AUTOPILOT CONTEXT (you are in autopilot mode):**
> - Your orchestrator is the phase-runner, NOT `/gsd:execute-phase`. Do not look for execute-phase workflow artifacts.
> - STATE.md may not exist. Use context from this prompt instead.
> - SUMMARY.md creation is optional. Include your summary in your response text. Create EXECUTION-LOG.md as instructed.

**Wait:** Poll until complete. This is the LONGEST step -- may take 10-30 minutes.

**Read back:** ONLY the SUMMARY section.

**Context cost:** ~15 lines.

---

### STEP 4: VERIFY -- MANDATORY INDEPENDENT AGENT

**Rules (unchanged):**
1. You MUST spawn a verifier subagent
2. You MUST NOT self-assess
3. The verifier reads the PLAN.md acceptance criteria, the actual file changes, and runs automated checks

**Action:** Spawn `gsd-verifier` agent via Task tool, run_in_background=false.

**Verifier prompt template:**

> You are an independent verifier. You did NOT write this code. Your job is to find problems.
>
> Phase: {N} -- {phase_name}
> Phase goal: {goal_from_roadmap}
> Phase type: {ui|protocol|data|mixed}
> Frozen spec at: {spec_path}
> Plans at: .planning/phases/{phase}/PLAN.md
> Changes since last checkpoint: Run `git diff {last_checkpoint_sha}..HEAD`
> Executor evidence: {paste the evidence section from executor's summary}
>
> **VERIFICATION METHODOLOGY (follow exactly):**
>
> **Step 1: Automated checks (ALL phase types):**
>
> Read `project.commands` from `.planning/config.json` for the actual commands to run.
>
> ```bash
> # 1. Compile check (run configured compile command)
> {project.commands.compile} 2>&1
> # Example for TypeScript: cd desktop-app && npx tsc --noEmit 2>&1
> # Record: PASS (0 errors) or FAIL (N errors, first 3 error messages)
>
> # 2. Lint check (run configured lint command)
> {project.commands.lint} 2>&1 | tail -5
> # Example for ESLint: cd desktop-app && npx eslint . --ext .ts,.tsx --quiet 2>&1 | tail -5
> # Record: PASS (0 errors) or FAIL (N errors)
> ```
>
> **Step 2: Phase-type-specific checks:**
>
> **If UI phase:**
> ```bash
> # 3. Production build (run configured build command)
> {project.commands.build} 2>&1
> # Record: PASS or FAIL with error
>
> # 4. Check that modified components are imported and rendered
> # For each component file in git diff, check it is imported somewhere:
> grep -r "import.*ComponentName" {project.ui.source_dir} --include="{project.ui.file_extensions}"
> # Record: IMPORTED or ORPHANED
> ```
>
> **If PROTOCOL phase:**
> ```bash
> # 3. Cross-reference validation: For each .md file modified:
> # Extract all file path references (backtick-wrapped paths to any file type)
> grep -oE '`[a-zA-Z0-9_./-]+\.[a-z]+`' {modified_file}
> # Verify each referenced file exists:
> ls -la {each_referenced_path} 2>&1
> # Record: all references valid OR list broken references
>
> # 4. Schema consistency: If file defines JSON contracts:
> # Extract JSON blocks and verify they parse
> # Verify field names match what consuming code expects
> ```
>
> **If DATA phase:**
> ```bash
> # 3. JSON validity
> python3 -c "import json; json.load(open('{data_file}'))" 2>&1
> # or: node -e "JSON.parse(require('fs').readFileSync('{data_file}','utf8'))"
> # Record: VALID or INVALID with error
>
> # 4. Schema compliance: Check required fields exist
> # Verify data files conform to their expected schema (project-specific)
> ```
>
> **If MIXED phase:**
> Run ALL checks from the applicable phase types above. A mixed phase combines UI + protocol
> or UI + data or all three. For each component of the mix, run that phase type's full checklist.
> Record results grouped by phase type.
>
> **Step 3: Acceptance criteria verification:**
> For EACH acceptance criterion in the PLAN.md:
> 1. Read the actual file at the path specified
> 2. Find the specific code/content that satisfies the criterion
> 3. Record: "Criterion: [text] -- VERIFIED at [file]:[line] -- [what you found]"
>    OR: "Criterion: [text] -- FAILED -- [what's missing or wrong]"
>
> Do NOT accept criteria based on executor claims. Read the files yourself.
>
> **Step 4: Cross-check executor evidence:**
> The executor provided evidence (commands run, file:line references).
> Spot-check at least 2 evidence claims by re-running the command or re-reading the file.
> Record whether executor evidence was accurate.
>
> Write report to: .planning/phases/{phase}/VERIFICATION.md
>
> At the END of your response, return JSON:
> ```json
> {
>   "pass": true|false,
>   "automated_checks": {
>     "compile": {"status": true|false, "detail": "0 errors" or "3 errors: ..."},
>     "lint": {"status": true|false, "detail": "..."},
>     "build": {"status": true|false|"n/a", "detail": "..."}
>   },
>   "criteria_results": [
>     {"criterion": "text", "status": "verified|failed", "evidence": "file:line -- what"}
>   ],
>   "executor_evidence_accurate": true|false,
>   "alignment_score": 1-10,
>   "failures": ["description"],
>   "scope_creep": ["anything built that was not in spec"]
> }
> ```
>
> **Scoring guide:**
> - 9-10: All criteria verified, all checks pass, no scope creep
> - 7-8: All criteria verified, minor issues (warnings, small scope creep)
> - 5-6: Most criteria verified, some failures or significant scope creep
> - 3-4: Multiple criteria failed, automated checks failing
> - 1-2: Fundamental misalignment, most criteria unmet
>
> **AUTOPILOT CONTEXT (you are in autopilot mode):**
> - Cross-check at least 2 of the executor's claims by re-reading the actual files.
> - Scoring guide: Do not default to 9/10. Scores of 7-8 with noted concerns are more credible than 9-10 with no concerns.

**Read back:** ONLY the JSON result.

**Context cost:** ~10 lines.

---

### STEP 4.5: LLM JUDGE -- MANDATORY INDEPENDENT AGENT

The judge provides an ADVERSARIAL second opinion. It does NOT read the verifier's conclusions first.

**Rules:**
1. You MUST spawn a judge subagent (subagent_type: "general-purpose")
2. The judge receives: (a) the frozen spec requirements, (b) the PLAN.md, (c) the raw git diff, (d) the executor's evidence
3. The judge does NOT receive the verifier's alignment score or pass/fail conclusion
4. The judge runs its OWN checks before reading the VERIFICATION.md
5. The judge MUST identify at least one concern (even if minor) to prove it examined the work

**Judge prompt template:**

> You are an adversarial judge. Your job is to find what the verifier MISSED.
>
> Phase: {phase_id} -- {phase_name}
> Phase type: {ui|protocol|data|mixed}
> Spec requirements for this phase: {requirements_list}
>
> **YOUR EVIDENCE (gather independently before reading any reports):**
>
> 1. Read the plan at: .planning/phases/{phase}/PLAN.md
> 2. Run: `git diff {last_checkpoint_sha}..HEAD --stat` (see what files changed and how much)
> 3. Run: `git log --oneline {last_checkpoint_sha}..HEAD` (see commit messages)
> 4. For each acceptance criterion in the plan, spot-check ONE by reading the actual file
> 5. Read the frozen spec at {spec_path} -- check if any spec requirement was missed
>
> **AFTER gathering your own evidence, read:**
> 6. .planning/phases/{phase}/VERIFICATION.md (the verifier's report)
>
> **Assess:**
> - Do YOUR findings agree with the verifier's conclusions?
> - Did the verifier miss anything you found?
> - Is the verifier's alignment score justified based on YOUR evidence?
> - Are there spec requirements with no implementation?
> - Is there code that nobody asked for (scope creep)?
>
> **MANDATORY: Identify at least one concern.** Even on perfect work, note something:
> a potential edge case, a missing test, a style inconsistency, a minor optimization.
> If you cannot find ANY concern, state: "No concerns found. Verified independently."
> with a description of what you checked to reach that conclusion.
>
> Return JSON:
> ```json
> {
>   "alignment_score": 1-10,
>   "recommendation": "proceed|debug|rollback|halt",
>   "concerns": ["at least one item"],
>   "verifier_agreement": true|false,
>   "verifier_missed": ["items the verifier didn't catch, if any"],
>   "scope_creep": ["items"],
>   "missing_requirements": ["items"],
>   "notes": "1-2 sentence assessment"
> }
> ```
>
> **Scoring guide (same as verifier -- scores should be INDEPENDENT):**
> - 9-10: All criteria verified, all checks pass, no scope creep
> - 7-8: All criteria verified, minor issues
> - 5-6: Most criteria verified, some failures
> - 3-4: Multiple criteria failed
> - 1-2: Fundamental misalignment

**Context cost:** ~5 lines (just the JSON).

---

### STEP 5: GATE DECISION

**Purpose:** YOU (the phase-runner) decide what happens next. This is your logic, not a subagent.

**Read:** The verify result (from STEP 4) and the judge result (from STEP 4.5).

**Decision tree:**

```
IF automated_checks all pass
   AND alignment >= 7
   AND recommendation == "proceed":
     -> Log decision: "Phase {N} PASSED. Proceeding."
     -> Go to STEP 6 (RESULT)
     -> Return status: "completed"

IF any automated_check fails OR recommendation == "debug":
     -> Log decision: "Phase {N} has failing checks. Entering debug loop."
     -> Go to STEP 5a (DEBUG)
     -> Max 3 debug attempts

IF alignment < 7 (and no automated failures):
     -> Log decision: "Phase {N} alignment too low ({score}). Re-planning."
     -> Go back to STEP 1 (RESEARCH) with tighter constraints
     -> Append to research prompt: "Previous attempt scored {alignment}/10. Issues: {issues}. Focus on: {missing}"
     -> Max 1 re-plan attempt per phase
     -> If second attempt also fails: return status "failed"

IF recommendation == "rollback":
     -> Log decision: "Phase {N} rollback recommended by judge. Reason: {reasoning}"
     -> Go to ROLLBACK (revert to last checkpoint)
     -> Return status "failed" with recommendation "rollback"

IF recommendation == "halt":
     -> Log decision: "Phase {N} HALTED by judge. Reason: {reasoning}"
     -> Go to ROLLBACK (revert to last checkpoint)
     -> Return status "failed" with recommendation "halt"

IF max retries exceeded (3 debug attempts OR 1 re-plan + 1 debug):
     -> Log decision: "Phase {N} max retries exceeded. Rolling back."
     -> Go to ROLLBACK
     -> Return status "failed" with recommendation "halt"
```

**ROLLBACK procedure:**

```bash
# Find the last checkpoint commit (provided by orchestrator)
LAST_GOOD_COMMIT={last_checkpoint_sha from inputs}

# Create a diagnostic branch before reverting
git branch autopilot-diagnostic-phase-{N}

# Revert to last good state (preserving history)
git revert --no-commit ${LAST_GOOD_COMMIT}..HEAD && git commit -m "rollback: revert to phase {N} checkpoint"
```

**CRITICAL: Never use `git reset --hard`. Use `git revert` to preserve history.**

**Context cost:** ~5 lines (just the decision + reason).

---

### STEP 5a: DEBUG (conditional)

**Purpose:** Fix failing automated checks or verification issues.

**Action:** Spawn `gsd-debugger` agent via Task tool, run_in_background=false.

**Prompt template:**

```
Debug phase {N}: {phase_name}.

Verification report at: .planning/phases/{phase}/VERIFICATION.md

Issues to fix:
{issues_list_from_verify_result}

Failing checks:
{automated_checks_that_failed}

Mode: find_and_fix
symptoms_prefilled: true

Fix the failing issues. Make atomic commits. Focus on the specific failures -- do NOT refactor unrelated code.

At the END, return JSON:
{
  "fixed": true|false,
  "changes": ["description of each fix"],
  "commits": ["sha1", "sha2"],
  "remaining_issues": ["anything still broken"]
}

**AUTOPILOT CONTEXT (you are in autopilot mode):**
- Your orchestrator is the phase-runner, NOT `/gsd:debug`. Do not wait for user input or checkpoints.
- STATE.md may not exist. Use context from this prompt instead.
- If you write code fixes, run compile and lint checks before committing.
- Return structured results: root cause, fix applied, commands run, file:line evidence.
```

**Circuit breaker:** Track debug loop count internally. Rules:

```
debug_attempt_1: Spawn debugger with full issue list
debug_attempt_2: Spawn debugger with remaining_issues from attempt 1
debug_attempt_3: Spawn debugger with remaining_issues from attempt 2

If attempt 3 returns fixed=false OR remaining_issues is non-empty:
  -> STOP. Do NOT attempt a 4th debug.
  -> Return status "failed" with recommendation "halt"
```

**After fix:** Return to STEP 4 (VERIFY) to confirm the fix worked.

**Context cost:** ~10 lines per attempt.

---

### STEP 6: COMPOSE RESULT

**Purpose:** Gather all data and compose the return JSON for the orchestrator.

**Actions (performed by YOU, the phase-runner -- NOT a subagent):**

1. **Record checkpoint commit:**
   ```bash
   CHECKPOINT_SHA=$(git rev-parse HEAD)
   ```

2. **Gather result data:**
   - Collect commit SHAs from execution
   - Collect verification/judge scores
   - Count debug and replan attempts
   - Summarize what was accomplished
   - **Compile evidence from executor summary and verifier results:**
     - `evidence.files_checked`: Merge executor's file:line references with verifier's criteria_results
     - `evidence.commands_run`: Merge executor's compile/lint/build results with verifier's automated checks
     - `evidence.git_diff_summary`: Run `git diff --stat {last_checkpoint_sha}..HEAD | tail -1`

3. **Compose return JSON** (see Section 4: Return Contract)

4. **Log progress:**
   ```
   Phase {N} complete. Alignment: {score}/10.
   ```

**Context cost:** ~5 lines.

---

## Section 3: Error Handling

When errors occur, include them in your return JSON (using the contract from orchestrator Section 4). The orchestrator handles user-facing communication and diagnostic file creation.

### Preflight Failures

Return immediately with: `status: "failed"`, `alignment_score: null`, `recommendation: "halt"`, `issues: ["Preflight failed: {reason}"]`. All task counts zero, no commits.

### Human-Verify Checkpoint

**Case 1 -- Mixed plan (auto + human-verify tasks):** Execute auto tasks, run verify/judge, then return with: `status: "needs_human_verification"`, populated `alignment_score`, `automated_checks`, and `commit_shas` from the auto tasks. Include `verification_request` describing what needs human approval.

**Case 2 -- Pure human-verify plan (zero auto tasks):** Skip execute/verify/judge. Return with: `status: "needs_human_verification"`, `alignment_score: null`, empty `automated_checks` and `commit_shas`. Set execute/verify/judge pipeline_steps to `"skipped"`.

### Pipeline Failures

After exhausting retries, return with: `status: "failed"`, `recommendation: "halt"`, populated `debug_attempts` count, all issues listed chronologically in `issues` array (original failure + each debug attempt result). Include `diagnostic_branch` name if one was created.

### Rollback Reporting

If rollback was performed, add to return: `rollback_performed: true`, `rollback_from: "{sha}"`, `rollback_to: "{sha}"`. Set `recommendation: "rollback"`.

---

## Section 4: Return Contract

The return contract is defined in `__INSTALL_BASE__/autopilot/protocols/autopilot-orchestrator.md` Section 4. Return that exact JSON structure as the LAST thing in your response. Key points:

- `pipeline_steps` uses shape: `{"status": "pass|fail|completed|skipped", "agent_spawned": boolean}`
- `evidence` field is REQUIRED for completed phases (see orchestrator Section 4)
- `alignment_score` is the JUDGE's score (not the verifier's)

---

## Quick Reference: Agent Spawn Table

| Step | Agent Type | Background? | Context Cost | Key Output |
|------|-----------|-------------|--------------|------------|
| 0 - Pre-flight | general-purpose | No | ~5 lines | JSON: all_clear |
| 1 - Research | gsd-phase-researcher | Yes | ~10 lines | SUMMARY section |
| 2 - Plan | gsd-planner | No | ~10 lines | SUMMARY section |
| 2.5 - Plan Check | gsd-plan-checker | No | ~5 lines | JSON: pass/issues |
| 3 - Execute | gsd-executor | Yes | ~15 lines | SUMMARY section |
| 4 - Verify | gsd-verifier | No | ~10 lines | JSON: pass/alignment |
| 4.5 - Judge | general-purpose | No | ~5 lines | JSON: alignment/recommendation |
| 5a - Debug | gsd-debugger | No | ~10 lines | JSON: fixed/remaining |

**Total per phase (happy path):** ~70 lines of context consumed.
**Total per phase (with 1 debug):** ~85 lines.

---

## Quick Reference: Decision Gates

| Gate | Pass Condition | Fail Action |
|------|---------------|-------------|
| Pre-flight | all_clear = true | Return failed result immediately |
| Plan Check | pass = true, confidence >= 7 | Re-plan (max 3x), then return failed |
| Verify + Judge | checks pass, alignment >= 7, recommendation = proceed | Debug (max 3x) or re-plan (max 1x), then return failed |
| Circuit Breaker | debug attempts < 3 | Return failed, recommend halt |

---

## Summary

This playbook defines the phase-runner subagent's step-specific behavior: exact prompt templates for each pipeline step, verification methodology with concrete bash commands per phase type, re-plan and debug loop logic, rollback procedures, and error handling. The phase-runner's identity, pipeline structure, context rules, and quality mindset are in its agent definition. The return contract is defined in `__INSTALL_BASE__/autopilot/protocols/autopilot-orchestrator.md` Section 4.
